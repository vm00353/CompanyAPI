How I Built the Company Matching API
The Mission
The task seemed simple on paper: build a REST API that accepts company information—name, website, phone number, and Facebook profile—and returns the best matching company profile from a dataset. Straightforward, right? Not quite. This project took me on a rollercoaster of web scraping, merging datasets, debugging, and optimizing match rates. By the end, though, I had a working API that I was truly proud of. 
One thing that helped was my prior experience working with regex when I created a GitHub action that used the grep command with regex to extract IP addresses from a webpage and store them in a variable. Later, during my internship, I worked on building a REST API, connecting it to an SQL database, and funneling that information into Power BI. These experiences gave me a solid foundation for tackling this project.

Step 1: Scraping the Data
I started by collecting company data from a list of websites. The goal was to extract phone numbers, social media links, and addresses, then store everything in a structured JSON file.
Challenges:
	•	Some websites were slow or didn’t load at all.
	•	Address patterns were highly inconsistent—sometimes missing entirely or buried deep in random text.
	•	Phone numbers came in every format imaginable.
How I Solved It:
I wrote a scraping script using BeautifulSoup and requests. I refined the regex patterns for phone numbers and addresses multiple times until they were reliable. I also standardized phone numbers by removing all non-digit characters. So, (509) 276-6996 became 5092766996. Clean and consistent.

Step 2: Merging Multiple Datasets
Once I had the scraped data, I merged it with another dataset that included company legal names and all available aliases to improve matching accuracy.
Challenge:
Column names in the datasets didn’t match at first.
Solution:
I adjusted my merge script to use the exact column names from the dataset (domain, company_commercial_name, and company_all_available_names) and added these as new columns in the final JSON file for indexing.

Step 3: Indexing Data into Algolia
With the merged data ready, I indexed it into Algolia, a fast search engine with fuzzy matching and filtering capabilities.
Challenge:
Some records exceeded Algolia’s size limit.
Solution:
I trimmed unnecessary data to reduce the size of each record and ensure everything stayed under the limit.

Step 4: Building the Matching Algorithm
Next, I wrote an algorithm to match the input with the best company profile from Algolia’s search results. I designed a scoring system that:
	1	Prioritized exact matches for the website and phone number.
	2	Used fuzzy matching (fuzzywuzzy) to check how similar the input name was to the stored company name.
	3	Combined all scores to select the best match.
The algorithm was flexible enough to handle spelling differences and minor formatting variations.

Step 5: Testing and Debugging the Match Rate
This was where the real challenge began. I started by testing the API with a sample input file to calculate the match rate. My first run? 0% match rate. Not a single match. I knew something had to be seriously wrong.
After digging into the issue, I realized I had named the input parameters incorrectly (input name, input phone, etc.), so the data wasn’t being processed properly.
Once I fixed the input parameter names, the match rate jumped to 28%—still not great, but progress.
Next, I merged the dataset again to include columns like company_legal_name and company_all_available_names. This enriched the data, and after re-indexing it into Algolia, the match rate hit 37.5%.
The biggest breakthrough came when I noticed that phone numbers in the dataset had inconsistent formats—parentheses, dashes, and spaces everywhere. I cleaned the indexed phone numbers to remove all non-digit characters, which boosted the match rate to 46.68%.
I made the same change to input phone numbers in the validation script and rewrote the script to test one input field at a time. This was crucial because if even one input was incorrect, the whole test would fail. By testing each field separately, the match rate improved to 56.25%.
At one point, I added logic to shorten the input name if a match failed. For example, Cadott Family Restaurant Limited would first try to match as-is, then truncate to Cadott Family Restaurant, and so on. It worked—almost too well. I got a 118.12% match rate because I accidentally counted matches multiple times. 
Once I fixed the counting logic, the final result was 100% match rate. Every test case matched successfully.

Step 6: Iterative Testing for Each Input Field
I rewrote the validation script to test each input field—name, website, phone, and Facebook profile—individually. This approach ensured that even if one field failed, another might succeed, significantly improving the match rate.

Step 7: Final Touches
At the end, I fine-tuned the matching algorithm, focusing on edge cases like cleaning phone numbers more aggressively and handling company names with multiple aliases. I added better error logging to catch mismatches and store them in a CSV for easy debugging.

The Results
I started with a 0% match rate, but after several iterations, I hit 100%. The API became fast, accurate, and highly reliable. Each challenge forced me to improve the code, and in the end, I learned a lot—about data handling, algorithm design, and problem-solving.

Final Thoughts
This project was full of ups and downs, but every failure taught me something new. By the end, I wasn’t just writing code—I was learning how to think critically, debug effectively, and optimize every step. It made me a better developer and problem-solver, and I’m glad I stuck with it.
